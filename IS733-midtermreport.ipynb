{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"IS733-midtermreport.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"xHKS-npzZdKU","colab_type":"code","outputId":"58e1baef-483c-438f-849f-d7b5ac8234bd","executionInfo":{"status":"ok","timestamp":1557174124116,"user_tz":240,"elapsed":2562,"user":{"displayName":"Munshi Mahbubur Rahman","photoUrl":"","userId":"14539240163568612566"}},"colab":{"base_uri":"https://localhost:8080/","height":84}},"source":["import pandas as pd\n","import re\n","import nltk\n","\n","nltk.download('punkt')\n","from nltk.tokenize import word_tokenize as wt \n","\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","\n","from nltk.stem.porter import PorterStemmer\n","stemmer = PorterStemmer()"],"execution_count":1,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-QTViwjhZdKZ","colab_type":"code","colab":{}},"source":["dataset = pd.read_json('/content/train.jsonl', lines=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qYsdXz8wZdKb","colab_type":"code","colab":{}},"source":["data = []\n","\n","for i in range(dataset.shape[0]):\n","    #print('i is', i)\n","    claim = dataset.iloc[i,0]\n","    #print('claim',claim)\n","\n","    # remove non alphanumeric characters\n","    claim = re.sub('[^A-Za-z0-9]', ' ', claim)\n","\n","    # make words lowercase, because Go and go will be considered as two words\n","    claim = claim.lower()\n","\n","    # tokenising\n","    tokenized_claim = wt(claim)\n","\n","    # remove stop words and stemming\n","    claim_processed = []\n","    for word in tokenized_claim:\n","        if word not in set(stopwords.words('english')):\n","            claim_processed.append(stemmer.stem(word))\n","\n","    claim_text = \" \".join(claim_processed)\n","    #print('claim_text is',claim_text)\n","    data.append(claim_text)\n","    #print('data is',data)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5MavvTvWZdKe","colab_type":"code","outputId":"a56830f3-bef2-4c5f-d495-6d107654fe7b","colab":{"base_uri":"https://localhost:8080/","height":286},"executionInfo":{"status":"ok","timestamp":1557174781711,"user_tz":240,"elapsed":9271,"user":{"displayName":"Munshi Mahbubur Rahman","photoUrl":"","userId":"14539240163568612566"}}},"source":["# creating the feature matrix with 1000 most of the frequent words occuring\n","from sklearn.feature_extraction.text import CountVectorizer\n","# matrix gets created with the claims as the rows and the selecetd features as the columns\n","matrix = CountVectorizer(max_features=1000)\n","X = matrix.fit_transform(data).toarray()\n","y = dataset.iloc[:, 3]\n","print('x shape',X.shape)\n","print('y shape',y.shape)\n","\n","# splitting data to create train and test data\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y)\n","\n","# training Naive Bayes model\n","from sklearn.naive_bayes import GaussianNB\n","classifier = GaussianNB()\n","classifier.fit(X_train, y_train)\n","\n","# predicting labels\n","y_pred = classifier.predict(X_test)\n","\n","# generating Confusion matrix\n","from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n","cm = confusion_matrix(y_test, y_pred)\n","cr = classification_report(y_test, y_pred)\n","\n","print('Confusion Matrix: ',cm)\n","print('Classification Report: ',cr)\n","\n","# calculating accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","print('Accuracy Score: ', accuracy)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["x shape (145449, 1000)\n","y shape (145449,)\n","Confusion Matrix:  [[3084 2663 3205]\n"," [1512 3717 2254]\n"," [4815 5706 9407]]\n","Classification Report:                   precision    recall  f1-score   support\n","\n","NOT ENOUGH INFO       0.33      0.34      0.34      8952\n","        REFUTES       0.31      0.50      0.38      7483\n","       SUPPORTS       0.63      0.47      0.54     19928\n","\n","      micro avg       0.45      0.45      0.45     36363\n","      macro avg       0.42      0.44      0.42     36363\n","   weighted avg       0.49      0.45      0.46     36363\n","\n","Accuracy Score:  0.44572780023650416\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-8XPnsokZdKi","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}